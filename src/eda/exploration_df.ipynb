{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, df1_name: str = \"DataFrame 1\", \n",
    "                      df2_name: str = \"DataFrame 2\", verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare columns between two DataFrames and identify differences.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df1 : pandas.DataFrame\n",
    "        First DataFrame to compare\n",
    "    df2 : pandas.DataFrame\n",
    "        Second DataFrame to compare\n",
    "    df1_name : str, default=\"DataFrame 1\"\n",
    "        Name of the first DataFrame for output\n",
    "    df2_name : str, default=\"DataFrame 2\"\n",
    "        Name of the second DataFrame for output\n",
    "    verbose : bool, default=True\n",
    "        If True, prints detailed comparison results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict containing:\n",
    "        - columns_only_in_df1: List of columns unique to df1\n",
    "        - columns_only_in_df2: List of columns unique to df2\n",
    "        - common_columns: List of columns present in both DataFrames\n",
    "        - dtype_differences: Dict of columns with different dtypes\n",
    "    \"\"\"\n",
    "    \n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "    \n",
    "    only_in_df1 = sorted(list(cols1 - cols2))\n",
    "    only_in_df2 = sorted(list(cols2 - cols1))\n",
    "    common_cols = sorted(list(cols1 & cols2))\n",
    "    \n",
    "    dtype_diff = {}\n",
    "    for col in common_cols:\n",
    "        if df1[col].dtype != df2[col].dtype:\n",
    "            dtype_diff[col] = {\n",
    "                'df1_dtype': str(df1[col].dtype),\n",
    "                'df2_dtype': str(df2[col].dtype)\n",
    "            }\n",
    "    \n",
    "    result = {\n",
    "        'columns_only_in_df1': only_in_df1,\n",
    "        'columns_only_in_df2': only_in_df2,\n",
    "        'common_columns': common_cols,\n",
    "        'dtype_differences': dtype_diff\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nDataFrame Column Comparison Results:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        print(f\"\\nColumns only in {df1_name} ({len(only_in_df1)}):\")\n",
    "        for col in only_in_df1:\n",
    "            print(f\"  - {col}\")\n",
    "            \n",
    "        print(f\"\\nColumns only in {df2_name} ({len(only_in_df2)}):\")\n",
    "        for col in only_in_df2:\n",
    "            print(f\"  - {col}\")\n",
    "            \n",
    "        print(f\"\\nCommon columns ({len(common_cols)}):\")\n",
    "        for col in common_cols:\n",
    "            print(f\"  - {col}\")\n",
    "            \n",
    "        if dtype_diff:\n",
    "            print(\"\\nColumns with different dtypes:\")\n",
    "            for col, dtypes in dtype_diff.items():\n",
    "                print(f\"  - {col}:\")\n",
    "                print(f\"      {df1_name}: {dtypes['df1_dtype']}\")\n",
    "                print(f\"      {df2_name}: {dtypes['df2_dtype']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def merge_datasets(baseline_df: pd.DataFrame, final_df: pd.DataFrame, \n",
    "                  merge_keys: list = ['NEFG2', 'LT_date'], validate_dtypes: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge Baseline and Final datasets using composite keys with validation and handling of duplicate columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    baseline_df : pandas.DataFrame\n",
    "        Baseline DataFrame\n",
    "    final_df : pandas.DataFrame\n",
    "        Final DataFrame\n",
    "    merge_keys : list, default=['NEFG2', 'LT_date']\n",
    "        List of columns to use as merge keys\n",
    "    validate_dtypes : bool, default=True\n",
    "        Whether to validate and harmonize dtypes before merging\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Merged DataFrame\n",
    "    \"\"\"\n",
    "    baseline = baseline_df.copy()\n",
    "    final = final_df.copy()\n",
    "    \n",
    "    for key in merge_keys:\n",
    "        if key not in baseline.columns or key not in final.columns:\n",
    "            raise ValueError(f\"Merge key '{key}' must exist in both DataFrames\")\n",
    "    \n",
    "    common_cols = [col for col in baseline.columns.intersection(final.columns) \n",
    "                  if col not in merge_keys]\n",
    "    \n",
    "    if validate_dtypes:\n",
    "        for col in common_cols:\n",
    "            if pd.api.types.is_numeric_dtype(baseline[col]) and \\\n",
    "               pd.api.types.is_numeric_dtype(final[col]):\n",
    "                baseline[col] = baseline[col].astype(float)\n",
    "                final[col] = final[col].astype(float)\n",
    "    \n",
    "    rename_dict = {col: f\"{col}_final\" for col in common_cols}\n",
    "    final = final.rename(columns=rename_dict)\n",
    "    \n",
    "    merged_df = pd.merge(baseline, final, on=merge_keys, how='outer', \n",
    "                        validate=None, indicator=True)\n",
    "    \n",
    "    merged_df['source'] = merged_df['_merge'].map({\n",
    "        'left_only': 'baseline_only',\n",
    "        'right_only': 'final_only',\n",
    "        'both': 'both'\n",
    "    })\n",
    "    \n",
    "    print(\"\\nMerge Summary:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Merge keys used: {', '.join(merge_keys)}\")\n",
    "    merge_counts = merged_df['source'].value_counts()\n",
    "    print(f\"Total rows in merged dataset: {len(merged_df)}\")\n",
    "    print(f\"Rows from baseline only: {merge_counts.get('baseline_only', 0)}\")\n",
    "    print(f\"Rows from final only: {merge_counts.get('final_only', 0)}\")\n",
    "    print(f\"Rows in both datasets: {merge_counts.get('both', 0)}\")\n",
    "    \n",
    "    for key in merge_keys:\n",
    "        null_counts = merged_df[key].isna().sum()\n",
    "        if null_counts > 0:\n",
    "            print(f\"\\nWarning: {null_counts} null values found in merge key '{key}'\")\n",
    "        \n",
    "        if key == 'NEFG2':\n",
    "            # Check for potential formatting differences in NEFG2\n",
    "            print(f\"\\nNEFG2 unique values sample in baseline:\")\n",
    "            print(baseline[key].value_counts().head())\n",
    "            print(f\"\\nNEFG2 unique values sample in final:\")\n",
    "            print(final[key].value_counts().head())\n",
    "    \n",
    "    merged_df = merged_df.drop('_merge', axis=1)\n",
    "    \n",
    "    for col in common_cols:\n",
    "        final_col = f\"{col}_final\"\n",
    "        if final_col in merged_df.columns:\n",
    "            merged_df[f\"{col}_combined\"] = np.where(\n",
    "                merged_df[final_col].notna(),\n",
    "                merged_df[final_col],\n",
    "                merged_df[col]\n",
    "            )\n",
    "\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "def analyze_merge_results(baseline_df: pd.DataFrame, final_df: pd.DataFrame, \n",
    "                         merged_df: pd.DataFrame, merge_key: str = 'LT_date', \n",
    "                         date_format: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the results of merging baseline and final datasets, focusing on unmatched cases.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    baseline_df : pandas.DataFrame\n",
    "        Original baseline DataFrame\n",
    "    final_df : pandas.DataFrame\n",
    "        Original final DataFrame\n",
    "    merged_df : pandas.DataFrame\n",
    "        Merged DataFrame containing 'source' column\n",
    "    merge_key : str, default='LT_date'\n",
    "        Column used as merge key\n",
    "    date_format : str, optional\n",
    "        Expected date format if merge_key is a date\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    def check_date_formatting(df, col):\n",
    "        \"\"\"Check for potential date formatting issues\"\"\"\n",
    "        try:\n",
    "            dates = pd.to_datetime(df[col])\n",
    "            return {\n",
    "                'min_date': dates.min(),\n",
    "                'max_date': dates.max(),\n",
    "                'null_dates': dates.isna().sum(),\n",
    "                'unique_formats': df[col].astype(str).drop_duplicates().tolist()[:5]\n",
    "            }\n",
    "        except:\n",
    "            return \"Date conversion failed\"\n",
    "\n",
    "    baseline_only = merged_df[merged_df['source'] == 'baseline_only']\n",
    "    final_only = merged_df[merged_df['source'] == 'final_only']\n",
    "    matched = merged_df[merged_df['source'] == 'both']\n",
    "    \n",
    "    date_analysis = {\n",
    "        'baseline': check_date_formatting(baseline_df, merge_key),\n",
    "        'final': check_date_formatting(final_df, merge_key)\n",
    "    }\n",
    "    \n",
    "    numeric_analysis = {}\n",
    "    for df_name, df in [('baseline_only', baseline_only), \n",
    "                       ('final_only', final_only), \n",
    "                       ('matched', matched)]:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_analysis[df_name] = {\n",
    "            col: {\n",
    "                'mean': df[col].mean(),\n",
    "                'median': df[col].median(),\n",
    "                'std': df[col].std(),\n",
    "                'null_count': df[col].isna().sum(),\n",
    "                'null_pct': (df[col].isna().sum() / len(df)) * 100\n",
    "            } for col in numeric_cols\n",
    "        }\n",
    "    \n",
    "    print(\"\\nDetailed Merge Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\n1. Basic Statistics:\")\n",
    "    print(f\"Total records in merged dataset: {len(merged_df)}\")\n",
    "    print(f\"Matched records: {len(matched)} ({len(matched)/len(merged_df)*100:.1f}%)\")\n",
    "    print(f\"Unmatched from baseline: {len(baseline_only)} ({len(baseline_only)/len(merged_df)*100:.1f}%)\")\n",
    "    print(f\"Unmatched from final: {len(final_only)} ({len(final_only)/len(merged_df)*100:.1f}%)\")\n",
    "    \n",
    "    if isinstance(date_analysis['baseline'], dict):\n",
    "        print(\"\\n2. Date Range Analysis:\")\n",
    "        print(\"\\nBaseline Dataset:\")\n",
    "        print(f\"Date range: {date_analysis['baseline']['min_date']} to {date_analysis['baseline']['max_date']}\")\n",
    "        print(f\"Null dates: {date_analysis['baseline']['null_dates']}\")\n",
    "        print(\"\\nFinal Dataset:\")\n",
    "        print(f\"Date range: {date_analysis['final']['min_date']} to {date_analysis['final']['max_date']}\")\n",
    "        print(f\"Null dates: {date_analysis['final']['null_dates']}\")\n",
    "    \n",
    "    print(\"\\n3. Unmatched Cases Analysis:\")\n",
    "    print(\"\\nBaseline-only cases:\")\n",
    "    print(baseline_only[merge_key].to_string())\n",
    "    print(\"\\nFinal-only cases:\")\n",
    "    print(final_only[merge_key].to_string())\n",
    "    \n",
    "    print(\"\\n4. Common Columns Value Distribution:\")\n",
    "    common_cols = set(baseline_df.columns) & set(final_df.columns)\n",
    "    for col in common_cols:\n",
    "        if col in numeric_analysis['matched']:\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            print(\"Mean values:\")\n",
    "            print(f\"  Matched cases: {numeric_analysis['matched'][col]['mean']:.2f}\")\n",
    "            if col in numeric_analysis['baseline_only']:\n",
    "                print(f\"  Baseline-only: {numeric_analysis['baseline_only'][col]['mean']:.2f}\")\n",
    "            if col in numeric_analysis['final_only']:\n",
    "                print(f\"  Final-only: {numeric_analysis['final_only'][col]['mean']:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'date_analysis': date_analysis,\n",
    "        'numeric_analysis': numeric_analysis,\n",
    "        'unmatched_baseline': baseline_only[merge_key].tolist(),\n",
    "        'unmatched_final': final_only[merge_key].tolist(),\n",
    "        'match_summary': {\n",
    "            'total': len(merged_df),\n",
    "            'matched': len(matched),\n",
    "            'unmatched_baseline': len(baseline_only),\n",
    "            'unmatched_final': len(final_only)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def investigate_unmatched_cases(baseline_df, final_df, merged_df):\n",
    "    \"\"\"\n",
    "    Wrapper function to analyze merge results and provide recommendations\n",
    "    \"\"\"\n",
    "    results = analyze_merge_results(baseline_df, final_df, merged_df)\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if isinstance(results['date_analysis']['baseline'], dict):\n",
    "        baseline_range = (results['date_analysis']['baseline']['min_date'],\n",
    "                         results['date_analysis']['baseline']['max_date'])\n",
    "        final_range = (results['date_analysis']['final']['min_date'],\n",
    "                      results['date_analysis']['final']['max_date'])\n",
    "        \n",
    "        if baseline_range != final_range:\n",
    "            print(\"\\n1. Date Range Mismatch Detected:\")\n",
    "            print(\"Consider investigating cases outside the overlapping date range\")\n",
    "    \n",
    "    if len(results['unmatched_baseline']) > 0:\n",
    "        print(\"\\n2. Baseline Unmatched Cases Pattern:\")\n",
    "        print(\"Review these specific dates for potential data entry issues\")\n",
    "    \n",
    "    if len(results['unmatched_final']) > 0:\n",
    "        print(\"\\n3. Final Unmatched Cases Pattern:\")\n",
    "        print(\"Review these specific dates for potential data entry issues\")\n",
    "    \n",
    "    print(\"\\nSpecific Actions:\")\n",
    "    print(\"1. Verify date formats are consistent between datasets\")\n",
    "    print(\"2. Check for any duplicate entries in either dataset\")\n",
    "    print(\"3. Investigate any significant value differences in common columns\")\n",
    "    print(\"4. Consider if unmatched cases should be excluded or require additional data collection\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_merged_dataset(merged_df, output_dir=\"./data\", \n",
    "                       prefix=\"merged\", include_timestamp=True):\n",
    "    \"\"\"\n",
    "    Save the merged dataset with appropriate documentation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    merged_df : pandas.DataFrame\n",
    "        The merged dataframe to save\n",
    "    output_dir : str, default=\"./data\"\n",
    "        Directory where to save the files\n",
    "    prefix : str, default=\"merged\"\n",
    "        Prefix for the output files\n",
    "    include_timestamp : bool, default=True\n",
    "        Whether to include timestamp in filenames\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if include_timestamp else \"\"\n",
    "    base_filename = f\"{prefix}_{timestamp}\" if timestamp else prefix\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}.csv\")\n",
    "    merged_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    metadata = {\n",
    "        'total_rows': len(merged_df),\n",
    "        'total_columns': len(merged_df.columns),\n",
    "        'columns_info': {col: str(dtype) for col, dtype in merged_df.dtypes.items()},\n",
    "        'missing_values': merged_df.isna().sum().to_dict(),\n",
    "        'source_distribution': merged_df['source'].value_counts().to_dict() if 'source' in merged_df.columns else None,\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, f\"{base_filename}_metadata.csv\")\n",
    "    pd.DataFrame([metadata]).to_csv(metadata_path, index=False)\n",
    "    \n",
    "    print(\"\\nMerged dataset saved successfully!\")\n",
    "    print(f\"Main dataset: {csv_path}\")\n",
    "    print(\"Metadata: {metadata_path}\")\n",
    "    print(\"\\nDataset summary:\")\n",
    "    print(f\"- Total rows: {metadata['total_rows']:,}\")\n",
    "    print(f\"- Total columns: {metadata['total_columns']}\")\n",
    "    if metadata['source_distribution']:\n",
    "        print(\"\\nSource distribution:\")\n",
    "        for source, count in metadata['source_distribution'].items():\n",
    "            print(f\"- {source}: {count:,} rows\")\n",
    "    \n",
    "    return {'data_path': csv_path, 'metadata_path': metadata_path, 'metadata': metadata}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Column Comparison Results:\n",
      "-----------------------------------\n",
      "\n",
      "Columns only in Baseline (52):\n",
      "  - CLAD_ddn\n",
      "  - CLAD_delay\n",
      "  - CLAD_delay_2\n",
      "  - CLAD_status\n",
      "  - CMV_DR\n",
      "  - DPG3_72h\n",
      "  - DSA_FINAL\n",
      "  - DSA_FINAL_noNA\n",
      "  - DSA_bin_1000\n",
      "  - DSA_bin_3000\n",
      "  - DSA_bin_500\n",
      "  - DSA_last\n",
      "  - DSA_last_date\n",
      "  - DSA_pre\n",
      "  - ECMO_postop\n",
      "  - ECMO_postop_duree_jours\n",
      "  - LAS\n",
      "  - LAS_2\n",
      "  - LAS_SU\n",
      "  - LT_age\n",
      "  - LT_age_2\n",
      "  - NEFG\n",
      "  - SU\n",
      "  - aetiology\n",
      "  - aetiology_2\n",
      "  - aetiology_3\n",
      "  - aetiology_3_and_1\n",
      "  - birth_date\n",
      "  - bmi\n",
      "  - center\n",
      "  - donneur_ISHLT\n",
      "  - donneur_PF\n",
      "  - donneur_age\n",
      "  - donneur_tabac\n",
      "  - donneur_tabac_2\n",
      "  - gender\n",
      "  - gender_2\n",
      "  - graft_survival_ddn\n",
      "  - graft_survival_delay\n",
      "  - graft_survival_delay_2\n",
      "  - graft_survival_status\n",
      "  - induction\n",
      "  - ischemie_duree\n",
      "  - natt\n",
      "  - nom_prenom_match\n",
      "  - patient_survival_ddn\n",
      "  - patient_survival_delay\n",
      "  - patient_survival_delay_2\n",
      "  - patient_survival_status\n",
      "  - re_tx\n",
      "  - taille\n",
      "  - type_tp\n",
      "\n",
      "Columns only in Final (9):\n",
      "  - date_prelevement\n",
      "  - delai_prelevement\n",
      "  - final_C4d\n",
      "  - final_C4d_distrib\n",
      "  - final_C4d_int\n",
      "  - final_infiltrat_A\n",
      "  - final_infiltrat_B\n",
      "  - protocol_cause\n",
      "  - time_point\n",
      "\n",
      "Common columns (11):\n",
      "  - LT_date\n",
      "  - NEFG2\n",
      "  - N_sans_P\n",
      "  - clust\n",
      "  - final_capillarite_agg\n",
      "  - final_fibrin_alv\n",
      "  - final_hyperplasie_pn\n",
      "  - final_infiltrat_A_A1_A2\n",
      "  - final_infiltrat_B_BNSP\n",
      "  - final_mPNPorg\n",
      "  - final_pn_alv\n",
      "\n",
      "Columns with different dtypes:\n",
      "  - N_sans_P:\n",
      "      Baseline: int64\n",
      "      Final: float64\n"
     ]
    }
   ],
   "source": [
    "baseline_df = pd.read_csv(\"../data/base_hcpc_baseline.csv\", sep=\";\")\n",
    "final_df = pd.read_csv(\"../data/base_hcpc_final.csv\", sep=\";\")\n",
    "differences = compare_dataframes(baseline_df, final_df, df1_name=\"Baseline\", df2_name=\"Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merge Summary:\n",
      "--------------------\n",
      "Merge keys used: NEFG2, LT_date\n",
      "Total rows in merged dataset: 15812\n",
      "Rows from baseline only: 19\n",
      "Rows from final only: 19\n",
      "Rows in both datasets: 15774\n",
      "\n",
      "NEFG2 unique values sample in baseline:\n",
      "NEFG2\n",
      "274444      13\n",
      "227421#1    13\n",
      "219015      12\n",
      "208792      12\n",
      "214571      12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "NEFG2 unique values sample in final:\n",
      "NEFG2\n",
      "274444      13\n",
      "227421#1    13\n",
      "215028#1    12\n",
      "214571      12\n",
      "208792      12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "merged_df = merge_datasets(baseline_df, final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Merge Analysis\n",
      "==================================================\n",
      "\n",
      "1. Basic Statistics:\n",
      "Total records in merged dataset: 15812\n",
      "Matched records: 15774 (99.8%)\n",
      "Unmatched from baseline: 19 (0.1%)\n",
      "Unmatched from final: 19 (0.1%)\n",
      "\n",
      "2. Date Range Analysis:\n",
      "\n",
      "Baseline Dataset:\n",
      "Date range: 2010-02-16 00:00:00 to 2019-11-01 00:00:00\n",
      "Null dates: 0\n",
      "\n",
      "Final Dataset:\n",
      "Date range: 2010-02-16 00:00:00 to 2019-11-01 00:00:00\n",
      "Null dates: 0\n",
      "\n",
      "3. Unmatched Cases Analysis:\n",
      "\n",
      "Baseline-only cases:\n",
      "3334     2018-07-31\n",
      "15320    2019-04-04\n",
      "15321    2019-04-04\n",
      "15322    2019-04-04\n",
      "15323    2019-04-04\n",
      "15324    2019-04-04\n",
      "15372    2019-03-26\n",
      "15373    2019-03-26\n",
      "15374    2019-03-26\n",
      "15375    2019-03-26\n",
      "15514    2019-04-19\n",
      "15515    2019-04-19\n",
      "15516    2019-04-19\n",
      "15517    2019-04-19\n",
      "15769    2019-06-25\n",
      "15770    2019-06-25\n",
      "15771    2019-06-25\n",
      "15772    2019-06-25\n",
      "15789    2019-08-21\n",
      "\n",
      "Final-only cases:\n",
      "3333     2012-04-13\n",
      "15315    2019-04-03\n",
      "15316    2019-04-03\n",
      "15317    2019-04-03\n",
      "15318    2019-04-03\n",
      "15319    2019-04-03\n",
      "15368    2019-03-25\n",
      "15369    2019-03-25\n",
      "15370    2019-03-25\n",
      "15371    2019-03-25\n",
      "15510    2019-04-17\n",
      "15511    2019-04-17\n",
      "15512    2019-04-17\n",
      "15513    2019-04-17\n",
      "15765    2019-06-24\n",
      "15766    2019-06-24\n",
      "15767    2019-06-24\n",
      "15768    2019-06-24\n",
      "15788    2019-08-20\n",
      "\n",
      "4. Common Columns Value Distribution:\n",
      "\n",
      "Column: clust\n",
      "Mean values:\n",
      "  Matched cases: 2.47\n",
      "  Baseline-only: 1.47\n",
      "  Final-only: nan\n",
      "\n",
      "Column: N_sans_P\n",
      "Mean values:\n",
      "  Matched cases: 1183.52\n",
      "  Baseline-only: 2389.95\n",
      "  Final-only: nan\n",
      "\n",
      "Recommendations:\n",
      "==================================================\n",
      "\n",
      "2. Baseline Unmatched Cases Pattern:\n",
      "Review these specific dates for potential data entry issues\n",
      "\n",
      "3. Final Unmatched Cases Pattern:\n",
      "Review these specific dates for potential data entry issues\n",
      "\n",
      "Specific Actions:\n",
      "1. Verify date formats are consistent between datasets\n",
      "2. Check for any duplicate entries in either dataset\n",
      "3. Investigate any significant value differences in common columns\n",
      "4. Consider if unmatched cases should be excluded or require additional data collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/sarrabenyahia/Documents/GitHub/longitudinal_BTB_clustering/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "analysis_results = investigate_unmatched_cases(baseline_df, final_df, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged dataset saved successfully!\n",
      "Main dataset: ../data/merged/baseline_final_merged_20241026_132423.csv\n",
      "Metadata: {metadata_path}\n",
      "\n",
      "Dataset summary:\n",
      "- Total rows: 15,812\n",
      "- Total columns: 91\n",
      "\n",
      "Source distribution:\n",
      "- both: 15,774 rows\n",
      "- baseline_only: 19 rows\n",
      "- final_only: 19 rows\n"
     ]
    }
   ],
   "source": [
    "save_info = save_merged_dataset(merged_df, \n",
    "                              output_dir=\"../data/merged\",\n",
    "                              prefix=\"baseline_final_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse améliorée des colonnes dupliquées:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: N_sans_P\n",
      "Type Baseline: float64\n",
      "Type Final: float64\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 0 (0.00%)\n",
      "\n",
      "Exemples de différences:\n",
      "   N_sans_P  N_sans_P_final\n",
      "0     943.0         17.2787\n",
      "1     943.0         18.1627\n",
      "2     944.0         17.2787\n",
      "3     944.0         18.1627\n",
      "4    1473.0         12.1070\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: clust\n",
      "Type Baseline: float64\n",
      "Type Final: float64\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 6483 (41.00%)\n",
      "\n",
      "Exemples de différences:\n",
      "   clust  clust_final\n",
      "1    1.0          2.0\n",
      "2    2.0          1.0\n",
      "5    2.0          1.0\n",
      "7    2.0          1.0\n",
      "8    1.0          2.0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: final_capillarite_agg\n",
      "Type Baseline: object\n",
      "Type Final: object\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 9472 (59.90%)\n",
      "\n",
      "Exemples de différences:\n",
      "     final_capillarite_agg final_capillarite_agg_final\n",
      "1  final_capillarite_agg_0     final_capillarite_agg_1\n",
      "2  final_capillarite_agg_1     final_capillarite_agg_0\n",
      "5  final_capillarite_agg_1     final_capillarite_agg_0\n",
      "7  final_capillarite_agg_1     final_capillarite_agg_0\n",
      "8  final_capillarite_agg_0     final_capillarite_agg_1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: final_fibrin_alv\n",
      "Type Baseline: object\n",
      "Type Final: object\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 14722 (93.11%)\n",
      "\n",
      "Exemples de différences:\n",
      "       final_fibrin_alv final_fibrin_alv_final\n",
      "99   final_fibrin_alv_0     final_fibrin_alv_1\n",
      "100  final_fibrin_alv_0     final_fibrin_alv_1\n",
      "103  final_fibrin_alv_1     final_fibrin_alv_0\n",
      "104  final_fibrin_alv_1     final_fibrin_alv_0\n",
      "107  final_fibrin_alv_1     final_fibrin_alv_0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: final_hyperplasie_pn\n",
      "Type Baseline: object\n",
      "Type Final: object\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 14246 (90.10%)\n",
      "\n",
      "Exemples de différences:\n",
      "       final_hyperplasie_pn final_hyperplasie_pn_final\n",
      "100  final_hyperplasie_pn_0     final_hyperplasie_pn_1\n",
      "106  final_hyperplasie_pn_0     final_hyperplasie_pn_1\n",
      "109  final_hyperplasie_pn_1     final_hyperplasie_pn_0\n",
      "110  final_hyperplasie_pn_1     final_hyperplasie_pn_0\n",
      "111  final_hyperplasie_pn_1     final_hyperplasie_pn_0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: final_infiltrat_A_A1_A2\n",
      "Type Baseline: object\n",
      "Type Final: object\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 12500 (79.05%)\n",
      "\n",
      "Exemples de différences:\n",
      "      final_infiltrat_A_A1_A2 final_infiltrat_A_A1_A2_final\n",
      "48  final_infiltrat_A_A1_A2_0     final_infiltrat_A_A1_A2_1\n",
      "52  final_infiltrat_A_A1_A2_0     final_infiltrat_A_A1_A2_1\n",
      "53  final_infiltrat_A_A1_A2_1     final_infiltrat_A_A1_A2_0\n",
      "54  final_infiltrat_A_A1_A2_1     final_infiltrat_A_A1_A2_0\n",
      "55  final_infiltrat_A_A1_A2_1     final_infiltrat_A_A1_A2_0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: final_infiltrat_B_BNSP\n",
      "Type Baseline: object\n",
      "Type Final: object\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 13696 (86.62%)\n",
      "\n",
      "Exemples de différences:\n",
      "       final_infiltrat_B_BNSP final_infiltrat_B_BNSP_final\n",
      "136  final_infiltrat_B_BNSP_0     final_infiltrat_B_BNSP_1\n",
      "138  final_infiltrat_B_BNSP_0     final_infiltrat_B_BNSP_1\n",
      "144  final_infiltrat_B_BNSP_0     final_infiltrat_B_BNSP_1\n",
      "146  final_infiltrat_B_BNSP_0     final_infiltrat_B_BNSP_1\n",
      "152  final_infiltrat_B_BNSP_0     final_infiltrat_B_BNSP_1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: final_mPNPorg\n",
      "Type Baseline: object\n",
      "Type Final: object\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 12776 (80.80%)\n",
      "\n",
      "Exemples de différences:\n",
      "      final_mPNPorg final_mPNPorg_final\n",
      "21  final_mPNPorg_1     final_mPNPorg_0\n",
      "22  final_mPNPorg_1     final_mPNPorg_0\n",
      "23  final_mPNPorg_1     final_mPNPorg_0\n",
      "24  final_mPNPorg_1     final_mPNPorg_0\n",
      "25  final_mPNPorg_0     final_mPNPorg_1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Colonne: final_pn_alv\n",
      "Type Baseline: object\n",
      "Type Final: object\n",
      "\n",
      "Résultats de la comparaison:\n",
      "Total des lignes: 15812\n",
      "Lignes identiques: 14736 (93.20%)\n",
      "\n",
      "Exemples de différences:\n",
      "       final_pn_alv final_pn_alv_final\n",
      "369  final_pn_alv_1     final_pn_alv_0\n",
      "370  final_pn_alv_0     final_pn_alv_1\n",
      "495  final_pn_alv_1     final_pn_alv_0\n",
      "496  final_pn_alv_0     final_pn_alv_1\n",
      "598  final_pn_alv_1     final_pn_alv_0\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def compare_duplicate_columns(merged_df, original_col_names):\n",
    "    \"\"\"\n",
    "    Compare values in columns that were duplicated during merge with improved string comparison\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    merged_df : pandas.DataFrame\n",
    "        The merged dataframe containing duplicate columns with suffixes\n",
    "    original_col_names : list\n",
    "        List of original column names (without suffixes)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    def clean_string_value(value):\n",
    "        \"\"\"Convert to string and strip any leading/trailing spaces\"\"\"\n",
    "        return str(value).strip()\n",
    "    \n",
    "    print(\"Analyse améliorée des colonnes dupliquées:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for col in original_col_names:\n",
    "        baseline_col = f\"{col}\"\n",
    "        final_col = f\"{col}_final\"\n",
    "        \n",
    "        if baseline_col in merged_df.columns and final_col in merged_df.columns:\n",
    "            # Get data types\n",
    "            baseline_type = merged_df[baseline_col].dtype\n",
    "            final_type = merged_df[final_col].dtype\n",
    "            \n",
    "            print(f\"\\nColonne: {col}\")\n",
    "            print(f\"Type Baseline: {baseline_type}\")\n",
    "            print(f\"Type Final: {final_type}\")\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(baseline_type) and pd.api.types.is_numeric_dtype(final_type):\n",
    "                is_equal = np.isclose(merged_df[baseline_col], merged_df[final_col], \n",
    "                                    rtol=1e-05, atol=1e-08, equal_nan=True)\n",
    "            else:\n",
    "                baseline_clean = merged_df[baseline_col].astype(str).apply(clean_string_value)\n",
    "                final_clean = merged_df[final_col].astype(str).apply(clean_string_value)\n",
    "                is_equal = baseline_clean == final_clean\n",
    "            \n",
    "            total_rows = len(merged_df)\n",
    "            matching_rows = is_equal.sum()\n",
    "            match_percentage = (matching_rows / total_rows) * 100\n",
    "            \n",
    "            print(\"\\nRésultats de la comparaison:\")\n",
    "            print(f\"Total des lignes: {total_rows}\")\n",
    "            print(f\"Lignes identiques: {matching_rows} ({match_percentage:.2f}%)\")\n",
    "            \n",
    "            if matching_rows < total_rows:\n",
    "                print(\"\\nExemples de différences:\")\n",
    "                mismatches = merged_df[~is_equal][[baseline_col, final_col]].head()\n",
    "                print(mismatches)\n",
    "                \n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "\n",
    "common_columns = [\n",
    "    'NEFG2',\n",
    "    'N_sans_P',\n",
    "    'clust',\n",
    "    'final_capillarite_agg',\n",
    "    'final_fibrin_alv',\n",
    "    'final_hyperplasie_pn',\n",
    "    'final_infiltrat_A_A1_A2',\n",
    "    'final_infiltrat_B_BNSP',\n",
    "    'final_mPNPorg',\n",
    "    'final_pn_alv'\n",
    "]\n",
    "\n",
    "compare_duplicate_columns(merged_df, common_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
